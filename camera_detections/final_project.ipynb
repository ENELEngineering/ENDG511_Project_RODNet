{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera-Based Detections\n",
    "\n",
    "This script will show the process for training, converting, and quantizing the camera-based\n",
    "detection portion of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import Parameters\n",
    "from tflite import TFliteRunner\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import glob\n",
    "import yaml\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Dataset\n",
    "\n",
    "The dataset can be downloaded from this link:\n",
    "https://universe.roboflow.com/vishwakarma-institute-of-technology-yqqb5/edi-ty/dataset/14/download\n",
    "\n",
    "1) Download the dataset following these options:\n",
    "\n",
    "![Dataset Download Options](images/dataset_download.png)\n",
    "\n",
    "2) Extract the dataset.\n",
    "\n",
    "3) Run the cells below to reformat the dataset, ensure the paths are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cells will reformat the dataset to be used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path pointing to the extracted dataset.\n",
    "extracted_dataset_path = \"EDI TY.v14i.yolov5pytorch\"\n",
    "if not os.path.exists(extracted_dataset_path):\n",
    "    raise FileNotFoundError(f\"The dataset is not found at: {extracted_dataset_path}\")\n",
    "\n",
    "# The path to save the formatted dataset.\n",
    "formatted_dataset_path = \"RoadDetectionDataset_v0\"\n",
    "if not os.path.exists(formatted_dataset_path):\n",
    "    os.makedirs(formatted_dataset_path)\n",
    "\n",
    "# Create images subdirectories\n",
    "os.makedirs(os.path.join(formatted_dataset_path, \"images/train\"))\n",
    "os.makedirs(os.path.join(formatted_dataset_path, \"images/validate\"))\n",
    "os.makedirs(os.path.join(formatted_dataset_path, \"images/test\"))\n",
    "# Create labels subdirectories\n",
    "os.makedirs(os.path.join(formatted_dataset_path, \"labels/train\"))\n",
    "os.makedirs(os.path.join(formatted_dataset_path, \"labels/validate\"))\n",
    "os.makedirs(os.path.join(formatted_dataset_path, \"labels/test\"))\n",
    "# Create dataset.yaml file\n",
    "contents = {\n",
    "    \"path\": os.path.abspath(formatted_dataset_path),\n",
    "    \"train\": os.path.abspath(os.path.join(formatted_dataset_path, \"images/train\")),\n",
    "    \"val\": os.path.abspath(os.path.join(formatted_dataset_path, \"images/validate\")),\n",
    "    \"test\": os.path.abspath(os.path.join(formatted_dataset_path, \"images/test\")),\n",
    "    \"names\": {\n",
    "        0: \"bike\",\n",
    "        1: \"car\",\n",
    "        2: \"cycle\",\n",
    "        3: \"pedestrian\",\n",
    "        4: \"signal\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(formatted_dataset_path, \"dataset.yaml\"), 'w') as outfile:\n",
    "    yaml.dump(contents, outfile, default_flow_style=False)\n",
    "\n",
    "with open(os.path.join(formatted_dataset_path, \"labels.txt\"), \"w\") as file:\n",
    "    file.write(\"bike\\n\")\n",
    "    file.write(\"car\\n\")\n",
    "    file.write(\"cycle\\n\")\n",
    "    file.write(\"pedestrian\\n\")\n",
    "    file.write(\"signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_images)=2801, len(validate_images)=802, len(test_images)=397\n",
      "len(train_labels)=2801, len(validate_labels)=802, len(test_labels)=397\n",
      "Moving test label 396 8010\r"
     ]
    }
   ],
   "source": [
    "train_images = glob.glob(os.path.join(extracted_dataset_path, \"train/images/*.jpg\"))\n",
    "validate_images = glob.glob(os.path.join(extracted_dataset_path, \"valid/images/*.jpg\"))\n",
    "test_images = glob.glob(os.path.join(extracted_dataset_path, \"test/images/*.jpg\"))\n",
    "print(f\"{len(train_images)=}, {len(validate_images)=}, {len(test_images)=}\")\n",
    "\n",
    "train_labels = glob.glob(os.path.join(extracted_dataset_path, \"train/labels/*.txt\"))\n",
    "validate_labels = glob.glob(os.path.join(extracted_dataset_path, \"valid/labels/*.txt\"))\n",
    "test_labels = glob.glob(os.path.join(extracted_dataset_path, \"test/labels/*.txt\"))\n",
    "print(f\"{len(train_labels)=}, {len(validate_labels)=}, {len(test_labels)=}\")\n",
    "\n",
    "# Move images\n",
    "for i, train_image in enumerate(train_images):\n",
    "    print(f\"Moving training image {i}\", end=\"\\r\")\n",
    "    new_path = os.path.abspath(os.path.join(formatted_dataset_path, f\"images/train/{os.path.basename(train_image)}\"))\n",
    "    shutil.move(train_image, new_path)\n",
    "\n",
    "for i, validate_image in enumerate(validate_images):\n",
    "    print(f\"Moving validate image {i}\", end=\"\\r\")\n",
    "    new_path = os.path.abspath(os.path.join(formatted_dataset_path, f\"images/validate/{os.path.basename(validate_image)}\"))\n",
    "    shutil.move(validate_image, new_path)\n",
    "\n",
    "for i, test_image in enumerate(test_images):\n",
    "    print(f\"Moving test image {i}\", end=\"\\r\")\n",
    "    new_path = os.path.abspath(os.path.join(formatted_dataset_path, f\"images/test/{os.path.basename(test_image)}\"))\n",
    "    shutil.move(test_image, new_path)\n",
    "\n",
    "# Move annotations\n",
    "for i, train_label in enumerate(train_labels):\n",
    "    print(f\"Moving training label {i}\", end=\"\\r\")\n",
    "    new_path = os.path.abspath(os.path.join(formatted_dataset_path, f\"labels/train/{os.path.basename(train_label)}\"))\n",
    "    shutil.move(train_label, new_path)\n",
    "\n",
    "for i, validate_label in enumerate(validate_labels):\n",
    "    print(f\"Moving validate label {i}\", end=\"\\r\")\n",
    "    new_path = os.path.abspath(os.path.join(formatted_dataset_path, f\"labels/validate/{os.path.basename(validate_label)}\"))\n",
    "    shutil.move(validate_label, new_path)\n",
    "\n",
    "for i, test_label in enumerate(test_labels):\n",
    "    print(f\"Moving test label {i}\", end=\"\\r\")\n",
    "    new_path = os.path.abspath(os.path.join(formatted_dataset_path, f\"labels/test/{os.path.basename(test_label)}\"))\n",
    "    shutil.move(test_label, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Convert the Model\n",
    "\n",
    "This portion of the project utilizes [YoloV5](https://github.com/ultralytics/yolov5). \n",
    "1) The repository can be cloned using this command.\n",
    "\n",
    "    ```shell\n",
    "    git clone https://github.com/ultralytics/yolov5.git\n",
    "    ```\n",
    "\n",
    "2) Once the repository is cloned, go to the repository and with a terminal, start\n",
    "the training process with the following command:\n",
    "\n",
    "    *Ensure to change the path to point to the dataset.yaml that is inside the formatted dataset.*\n",
    "\n",
    "    ```shell\n",
    "    python train.py --data=<path to the dataset.yaml> --epochs=20\n",
    "    ```\n",
    "\n",
    "    *After training, the model files will be found under `/runs/train/exp<n>`*\n",
    "\n",
    "3) The next step is to perform conversion of the model to TFLite and TensorFlow saved model using this command.\n",
    "\n",
    "    ```shell\n",
    "    python export.py --data=<path to the dataset.yaml> --weights=<path to the trained .pt model> --keras --include=[\"saved_model\", \"tflite\"]\n",
    "    ```\n",
    "\n",
    "4) The next step is to take the float16 saved model and quantize it to Int8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Dynamic Quantization of the Model\n",
    "\n",
    "The cell below performs post training dynamic quantization of the float16 Tensorflow Saved Model into Int8 TFLite model.\n",
    "\n",
    "After conversion, YoloV5 will output a tensorflow saved_model with the directory structure shown below:\n",
    "\n",
    "```\n",
    "saved_model-keras\n",
    "        |---------- assets\n",
    "        |---------- variables\n",
    "        |---------- fingerprint.pb\n",
    "        |---------- keras_metadata.pb\n",
    "        |---------- saved_model.pb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path to point to the saved_model directory.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"epoch-20-BMPCS-best_saved_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save TFLite Model\n",
    "with open('epoch-20-BMPCS-best-BMPCS-_quantized_saved_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "1) Validation of the models utilizes `deepview-validator` which can be installed using\n",
    "\n",
    "    ```shell\n",
    "    pip install deepview-validator\n",
    "    ```\n",
    "\n",
    "2) Due to current limitations in the validator software, move the `dataset.yaml` outside the dataset directory for now.\n",
    "\n",
    "3) This is the command template used to validate the models once validator is installed.\n",
    "\n",
    "    ```shell\n",
    "    deepview-validator <path to the tflite model> --dataset=<path to the formatted dataset> --norm=unsigned --visualize=<path to save the visualization of results and metrics>\n",
    "    ```\n",
    "\n",
    "*Note: Sample commands are provided below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Int Model\n",
    "\n",
    "```shell\n",
    "deepview-validator epoch-20-BMPCS-best-BMPCS-_quantized_saved_model.tflite --dataset=RoadDetectionDataset_v0 --norm=unsigned --visualize=results\n",
    "```\n",
    "\n",
    "### Float Model\n",
    "\n",
    "```shell\n",
    "deepview-validator epoch-20-BMPCS-best-fp16.tflite --dataset=RoadDetectionDataset_v0 --norm=unsigned --visualize=results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "Here is a demo for the camera based detection. A video is also available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "parameters = Parameters(\n",
    "        detection_score=0.001,\n",
    "        detection_iou=0.60,\n",
    "        acceptance_score=0.40,\n",
    "        max_detections=300,\n",
    "        normalization=\"unsigned\",\n",
    "        warmup=0\n",
    "    )\n",
    "# Model runner\n",
    "loaded_model = TFliteRunner(\n",
    "    model=\"epoch-20-BMPCS-best-fp16.tflite\",\n",
    "    parameters=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize\n",
    "\n",
    "# define a video capture object \n",
    "vid = cv2.VideoCapture(0) \n",
    "  \n",
    "while(True): \n",
    "      \n",
    "    # Capture the video frame by frame \n",
    "    ret, frame = vid.read() \n",
    "  \n",
    "    boxes, scores, labels = loaded_model.detect(frame)\n",
    "    frame = visualize(frame, boxes, scores, labels)\n",
    "    \n",
    "    # Display the resulting frame \n",
    "    cv2.imshow('frame', frame) \n",
    "\n",
    "    # the 'q' button is set as the \n",
    "    # quitting button you may use any \n",
    "    # desired button of your choice \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
